# MuZero Experiments 🤖

In 2016, Deepmind introduced [AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far), the first artificial intelligence (AI) program to defeat humans at the ancient game of Go. Two years later, its successor - [AlphaZero](https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go) - learned from scratch to master Go, chess and shogi. Now, in a paper we choose to study, [Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://arxiv.org/pdf/1911.08265.pdf), Deepmind describes MuZero, a significant step forward in the pursuit of general-purpose algorithms. MuZero masters Go, chess, shogi and Atari without needing to be told the rules, thanks to its ability to plan winning strategies in unknown environments (credits to the excellent [Deepmind blog](https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules)).

## Introduction and Disclaimer 🎓

This reprository is groups of methods to assess and compare DeepMind's MuZero algorithm.
It was created in the context of the MAP670C - Reinforcement Learning (2020-2021) course with l'Ecole Polytechnique.
The majority of the code was forked and adapted for demo from [MuZero General](https://github.com/werner-duvaud/muzero-general) and [AlphaZero for Connect4](https://github.com/jpbruneton/Alpha-Zero-algorithm-for-Connect-4-game).

## A set of demos and comparisons of MuZero 🔥

### CartPole and LunarLander 🚀

### Board Games ♟️

### Comparison to AlphaZero and Minimax with alpha-beta pruning 🐍
